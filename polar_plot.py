##polar_plot.py

#from psychopy import visual
#from psychopy import core
#from psychopy import info
#import TDT_control
import matplotlib.pyplot as plt
import h5py
import numpy as np
#from multiprocessing.pool import ThreadPool
import pickle

##grating variables
#directions of movement (list). -1 is the baseline gray screen
##***NOTE: for plotting to be correct, the gray screen value should be last in the array!!!***
DIRECTIONS = np.array([0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, -1])
DIRECTIONS2 = np.array([0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360, -1])
##contrast values (list). -1 is the baseline gray screen
CONTRASTS = np.array([1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, -1])
#the spatial frequency of the grating in cycles/degree
SPATIAL_FREQ = 0.04
##amount of time to display the gray screen in seconds
GRAY_TIME = 2.0
##amount of time to display the drifting grating in seconds
DRIFT_TIME = 4.0
#the number of times to repeat the full set of gratings
NUM_SETS = 8
##the location of the TDT circuit file to load
CIRCUIT_LOC = r"C:\TDT\OpenEx\MyProjects\visual_tuning_2\RCOCircuits\V1_tuning_circuit.rcx"
##the location to save the data
SAVE_LOC = r"C:\Users\TDT-RAT\Desktop\testFile.hdf5"
##the location to save the plot data
DICT1_LOC = r"C:\Users\TDT-RAT\Desktop\testFile1.hdf5"
DICT2_LOC = r"C:\Users\TDT-RAT\Desktop\testFile2.hdf5"
##create an HDF5 file in which to save the data
#dataFile = h5py.File(SAVE_LOC, 'w-')
##load the spcified circuit file and connect to the processor
"""
NOTE: Loading the wrong file isn't immediately obvious and can cause
a lot of headaches!!
"""
#RZ2 = TDT_control.RZ2(CIRCUIT_LOC)
##load the RPvdsEx circuit locally
#RZ2.load_circuit(local = True, start = False)
##get the processor sampling rate
fs = 24414.0625
##the number of samples to take from the TDT (duration of each stim rep)
num_samples = int(np.ceil(fs*(DRIFT_TIME+2*GRAY_TIME)))

##a function to parse the sort code data generated by the PC Sort macro in RPvdsEx
def parse_sorted(arr_in):
	##lookup table for unit letters to be compatible with plexon
	unit_letters = ['a', 'b', 'c', 'd']
	##generate an output dictionary
	results_dict = {}
	##how many channels are we dealing wtih?
	num_chans = arr_in.shape[0]
	##sometimes the TDT outputs a '31.0-' not sure if this is some kind of bug or what
	##but I'm just going to get rid of them
	#np.place(arr_in, arr_in==31, [0])
	##run through each channel and parse individual sort codes
	for c in range(int(num_chans)):
		#how many units sorted on this channel?
		num_sorted = arr_in[c,:].max()
		##if the max is > 4, something's wrong, so deal with it
		while num_sorted > 4:
			#print "Removing outlier sort code ("+str(num_sorted)+")"
			np.place(arr_in[c,:], arr_in[c,:]==num_sorted, [0])
			num_sorted = arr_in[c,:].max()
		if num_sorted > 0:
			for unit in range(1,int(num_sorted+1)):
				spiketrain = (arr_in[c,:] == unit).astype(int)
				if c < 9:
					pad = '00'
				elif c < 99:
					pad = '0'
				else:
					pad = ''
				name = 'sig'+pad+str(c+1)+unit_letters[unit-1]
				results_dict[name] = spiketrain
	return results_dict


## a helper function to get the names of all the sorted units in a file set
def get_sorted_names(fIn):
	##get the names from one set as a start
	unit_names = parse_sorted(np.asarray(fIn['orientation']['set_1']['0'][0,:,:])).keys()
	##check all instances, and if there is a new addition, add it to the master list
	for setN in range(NUM_SETS):
		for oriN in DIRECTIONS:
			units_present = parse_sorted(np.asarray(fIn['orientation']['set_'+str(setN+1)][str(oriN)][0,:,:])).keys()
			for unit in units_present:
				if unit not in unit_names:
					print "adding a unit - " + unit
					unit_names.append(unit)
	return unit_names

##a function to generate polar plots and save data for direction tuning
def plot_direction_tuning(fIn):
	##first, just figure out how many sorted units there are.
	unit_names = get_sorted_names(fIn)
	num_units = len(unit_names)
	##decide what sample range corresponds to the stimuli presentation
	drift_samples = np.arange(np.floor(fs*GRAY_TIME),np.ceil(fs*GRAY_TIME+fs*DRIFT_TIME)).astype(int)
	##make a data dictionary; allocate memory
	tuning_dict = {}
	for name in unit_names:
		tuning_dict[name] = np.zeros((DIRECTIONS.size, drift_samples.size))
	##go through each set
	for nSet in range(NUM_SETS):
		##...and each unit
		for name in unit_names:
			##...and each angle
			## add the data from each direction to the dictionary
			for n, angle in enumerate(DIRECTIONS):
				data = parse_sorted(np.asarray(fIn['orientation']['set_'+str(nSet+1)][str(angle)][0,:,:]))
				try:
					tuning_dict[name][n,:] += data[name][drift_samples]
				except KeyError:
					##if the parsing function didn't pick up this unit on this trial, it means there were no spikes
					##so just "add" zeros 
					"no spikes for unit " + name 
					pass
	##do some more work to normalize the values
	norm_dict = {}
	for name in unit_names:
		current_set = tuning_dict[name]
		##calculate the spike rates
		rates = []
		for d in range(current_set.shape[0]):
			rates.append(float(current_set[d,:].sum())/DRIFT_TIME/NUM_SETS)
		rates = np.asarray(rates)
		##normalize everything to the control rate (assuming here that it's the last in the list)
		if rates[-1] != 0:
			rates = rates/rates[-1]
		else: 
			print "rate for control is zero."
		norm_dict[name] = rates
	##save the data dictionaries with pickle
	pickle.dump(tuning_dict, open(DICT1_LOC, 'wb'))
	pickle.dump(tuning_dict, open(DICT2_LOC, 'wb'))
	##plot everything! (finally)
	for name in unit_names:
		plt.figure()
		ax = plt.subplot(111, polar = True)
		ax.plot(np.radians(DIRECTIONS[0:-1]), norm_dict[name][0:-1])
		ax.set_rmax(norm_dict[name].max())
		ax.grid(True)
		ax.set_title(name)
	plt.show()

##a function to convert point-binned spike trains to an array of timestamps
def pb_to_ts(spiketrain):
	result = np.where(spiketrain == 1)
	return result

##a function to generate raster plots for a given unit and a given direction
#returns an axis containing the raster plot
def plot_rasters(fPre, fPost, orientation, unit_name):
	#get a list of all the sets in the file
	num_sets = ['set_1', 'set_2']
	#fPre['orientation'].keys()
	##allocate memory for the final datasets
	data1 = np.zeros((len(num_sets), num_samples))
	data2 = np.zeros((len(num_sets), num_samples))
	##parse the relevant data from the file
	for n, setName in enumerate(num_sets):
		spiketrains1 = parse_sorted(np.asarray(fPre['orientation'][setName][orientation][0,:,:]))
		spiketrains2 = parse_sorted(np.asarray(fPost['orientation'][setName][orientation][0,:,:]))
		try:
			spiketrain1 = spiketrains1[unit_name]
			data1[n,0:spiketrain1.shape[0]] = spiketrain1
		except KeyError:
			pass
		try:
			spiketrain2 = spiketrains2[unit_name]
			data2[n,0:spiketrain2.shape[0]] = spiketrain2
		except KeyError:
			pass
	##now plot the raster
	fig, (ax1, ax2) = plt.subplots(1,2,sharey = True)
	for i in range(len(num_sets)):
		ax1.vlines(np.asarray(np.where(data1[i,:]==1))/fs, i+0.5, i+1.5, color = 'b')
		ax2.vlines(np.asarray(np.where(data2[i,:]==1))/fs, i+0.5, i+1.5, color = 'r')
	ax1.set_ylim(0.5, len(num_sets)+0.5)
	ax2.set_ylim(0.5, len(num_sets)+0.5)
	ax1.axvspan(GRAY_TIME, GRAY_TIME+DRIFT_TIME, color = 'g', alpha = 0.5)
	ax2.axvspan(GRAY_TIME, GRAY_TIME+DRIFT_TIME, color = 'g', alpha = 0.5)
	ax1.set_xlim(0,2*GRAY_TIME+DRIFT_TIME)
	ax2.set_xlim(0,2*GRAY_TIME+DRIFT_TIME)
	ax1.set_xlabel('time, sec')
	ax2.set_xlabel('time, sec')
	ax1.set_ylabel('trials')
	fig.suptitle('Raster plot for '+unit_name+' at '+orientation+' degrees ', fontsize = 20)
	ax1.set_title('Before sham BMI')
	ax2.set_title('After sham BMI')
	ax1.text(2.5, 2.5, 'display period')
	ax2.text(2.5, 2.5, 'display period')
	ax1.yaxis.set_ticks(np.array([1,2,3,4]))
	plt.show()


##a function to plot all of the raster plots for a given unit in a given session
def plot_all_rasters(fPre, fPost, unit_name):
	num_orientations = fPre['orientation']['set_1'].keys()
	for n, ori in enumerate(num_orientations):
		plot_rasters(fPre, fPost, ori, unit_name)


##plots comparison polar plots before and after training
def plot_comparisons(fPre, fPost):
	##first, just figure out how many sorted units there are.
	unit_names = get_sorted_names(fPre)
	num_units = len(unit_names)
	##decide what sample range corresponds to the stimuli presentation
	drift_samples = np.arange(np.floor(fs*GRAY_TIME),np.ceil(fs*GRAY_TIME+fs*DRIFT_TIME)).astype(int)
	##make a data dictionary; allocate memory
	tuning_dict_pre = {}
	tuning_dict_post = {}
	for name in unit_names:
		tuning_dict_pre[name] = np.zeros((DIRECTIONS.size, drift_samples.size))
		tuning_dict_post[name] = np.zeros((DIRECTIONS.size, drift_samples.size))
	##go through each set
	for nSet in range(NUM_SETS):
		##...and each unit
		for name in unit_names:
			##...and each angle
			## add the data from each direction to the dictionary
			for n, angle in enumerate(DIRECTIONS):
				data_pre = parse_sorted(np.asarray(fPre['orientation']['set_'+str(nSet+1)][str(angle)][0,:,:]))
				data_post = parse_sorted(np.asarray(fPost['orientation']['set_'+str(nSet+1)][str(angle)][0,:,:]))
				try:
					tuning_dict_pre[name][n,:] += data_pre[name][drift_samples]
				except KeyError:
					##if the parsing function didn't pick up this unit on this trial, it means there were no spikes
					##so just "add" zeros 
					"no spikes for unit " + name 
					pass
				try:
					tuning_dict_post[name][n,:] += data_post[name][drift_samples]
				except KeyError:
					##if the parsing function didn't pick up this unit on this trial, it means there were no spikes
					##so just "add" zeros 
					"no spikes for unit " + name 
					pass
	##do some more work to normalize the values
	norm_dict_pre = {}
	norm_dict_post = {}
	for name in unit_names:
		current_set_pre = tuning_dict_pre[name]
		current_set_post = tuning_dict_post[name]
		##calculate the spike rates
		rates_pre = []
		rates_post = []
		for d in range(current_set_pre.shape[0]):
			rates_pre.append(float(current_set_pre[d,:].sum())/DRIFT_TIME/NUM_SETS)
			rates_post.append(float(current_set_post[d,:].sum())/DRIFT_TIME/NUM_SETS)
		rates_pre.insert(-1,rates_pre[0])
		rates_pre = np.asarray(rates_pre)
		rates_post.insert(-1,rates_post[0])
		rates_post = np.asarray(rates_post)
		##normalize everything to the control rate (assuming here that it's the last in the list)
		if rates_pre[-1] != 0:
			rates_pre = rates_pre/rates_pre[-1]
		else: 
			print "rate for control is zero."
		norm_dict_pre[name] = rates_pre
		if rates_post[-1] != 0:
			rates_post = rates_post/rates_post[-1]
		else: 
			print "rate for control is zero."
		norm_dict_post[name] = rates_post
	##save the data dictionaries with pickle
	#pickle.dump(tuning_dict, open(DICT1_LOC, 'wb'))
	#pickle.dump(tuning_dict, open(DICT2_LOC, 'wb'))
	##plot everything! (finally)
	for name in unit_names:
		plt.figure()
		ax1 = plt.subplot(111,polar=True)
		ax1.plot(np.radians(DIRECTIONS2[0:-1]), norm_dict_pre[name][0:-1], label = 'pre-training', color = 'b')
		ax1.plot(np.radians(DIRECTIONS2[0:-1]), norm_dict_post[name][0:-1], label = 'post_training', color = 'r')
		ax1.set_rmax(max(norm_dict_pre[name].max(), norm_dict_post[name].max()))
		#ax2.set_rmax(norm_dict_post[name].max())
		ax1.grid(True)
		#ax2.grid(True)
		ax1.set_title('Direction tuning for unit '+name)
		ax1.legend()
		#ax2.set_title('post-training')
		#fig.suptitle('Direction tuning for unit '+name, fontsize = 20)
	plt.show()